{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd external/YOLOX/\n",
    "%pip install -r requirements.txt && python setup.py develop\n",
    "\n",
    "%cd ../external/deep-person-reid/\n",
    "%pip install -r requirements.txt && python setup.py develop\n",
    "\n",
    "%cd ../external/fast_reid/\n",
    "%pip install -r docs/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mv /home/Deep-OC-SORT/external/weights/* /home/share/model/DeepOC-SORT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python setup.py develop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /home/share/data_warehouse/MOTChallenge/MOT17-FRCNN.zip -d /home/share/datasets/mot\n",
    "!unzip /home/share/data_warehouse/MOTChallenge/MOT20.zip -d /home/share/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转为COCO格式\n",
    "# replace \"dance\" with mot17/mot20 for others\n",
    "!python data/tools/convert_dance_to_coco.py "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%exp=baseline\n",
    "# Flags to disable all the new changes\n",
    "!python main.py --exp_name $exp --post --emb_off --cmc_off --aw_off --new_kf_off --grid_off --dataset mot17\n",
    "!python main.py --exp_name $exp --post --emb_off --cmc_off --aw_off --new_kf_off --grid_off -dataset mot20 --track_thresh 0.4\n",
    "!python main.py --exp_name $exp --post --emb_off --cmc_off --aw_off --new_kf_off --grid_off --dataset dance --aspect_ratio_thresh 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%exp=best_paper_ablation\n",
    "!python main.py --exp_name $exp --post --grid_off --new_kf_off --dataset mot17 --w_assoc_emb 0.75 --aw_param 0.5\n",
    "!python main.py --exp_name $exp --post --grid_off --new_kf_off --dataset mot20 --track_thresh 0.4 --w_assoc_emb 0.75 --aw_param 0.5\n",
    "!python main.py --exp_name $exp --post --grid_off --new_kf_off --dataset dance --aspect_ratio_thresh 1000 --w_assoc_emb 1.25 --aw_param 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ArTIST\n",
    "%exp=artist_mot17_eval\n",
    "!python main.py --exp_name $exp --emb_off --cmc_off --aw_off --grid_off --new_kf_off --dataset mot17_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python external/TrackEval/scripts/run_mot_challenge.py \\\n",
    "  --SPLIT_TO_EVAL val \\\n",
    "  --METRICS HOTA CLEAR Identity \\\n",
    "  --TRACKERS_TO_EVAL best_paper_ablations \\\n",
    "  --GT_FOLDER results/gt/ \\\n",
    "  --TRACKERS_FOLDER results/trackers/ \\\n",
    "  --BENCHMARK DANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --exp_name DeepOC_mot17-test --dataset mot17 --test_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOT17-02-FRCNN\n",
      "1920\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"/home/share/datasets/mot/annotations/val_half.json\"\n",
    "with open(file_path, 'r') as fp:\n",
    "    img_info = json.load(fp)\n",
    "    seqs = [i['file_name'] for i in img_info['videos']]\n",
    "    img_path = {}\n",
    "    images = img_info['images']\n",
    "    img0 = images[0]\n",
    "    img_seq = img0['file_name'].split('/')[0]\n",
    "    print(img_seq)\n",
    "    print(img_info['images'][0]['width'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 2])\n",
      "torch.Size([1, 3, 2])\n",
      "torch.Size([3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[[1,2]],\n",
    "                  [[3,4]],\n",
    "                  [[5,6]]])\n",
    "print(a.shape)\n",
    "b = a.permute(1,0,2)\n",
    "c = b.permute(1,0,2)\n",
    "print(b.shape)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2,3,4],\n",
    "              [5,6,7,8]])\n",
    "b = torch.from_numpy(a)\n",
    "\n",
    "print(torch.sum(b>50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
